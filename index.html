<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>SVT</title>
	<meta property="og:image" content="Path to my teaser.png"/>
	<meta property="og:title" content="Peekaboo" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
<!--	Header with title and authors-->
	<div style="text-align: center;">
		<span style="font-size:42px">Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors</span>
		<br> <br>
		<table align=center width=800px>
			<table align=center width=500px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://ryanndagreat.github.io">Ryan Burgert</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=K2WBZTwAAAAJ">Kanchana Ranasinghe</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=qkyC7KQAAAAJ&hl=en">Xiang Li</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=vcw0TJIAAAAJ&hl=en">Michael Ryoo</a></span>
						</center>
					</td>
				</tr>
			</table>
		<br>
			<table align=center width=400px>
				<tr>
					<td align=center width=120px>
						<div style="text-align: center;">
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2211.13224'>[Paper]</a></span>
						</div>
					</td>
					<td align=center width=120px>
						<div style="text-align: center;">
							<span style="font-size:24px"><a href='https://github.com/RyannDaGreat/Peekaboo'>[GitHub]</a></span><br>
						</div>
					</td>
					<td align=center width=120px>
						<div style="text-align: center;">
							<span style="font-size:24px"><a href=''>[Slides]</a></span><br>
						</div>
					</td>
				</tr>
			</table>
		</table>
		<br>
	</div>

<!--	Intro image and description-->
	<div style="text-align: center;">
		<table align=center width=850px>
			<tr>

				<td width=600px>
					<div style="text-align: center;">
						<img class="round" style="width:600px" src="./resources/intro.png"/>
					</div>
				</td>
			</tr>
		</table>
		<table align=center width=600px>
			<tr>
				<td>
					<div style="text-align: justify;">
<!--						<b>Zero-Shot Segmentation with phrases:</b> We highlight-->
<!--						the ability of proposed Peekaboo to ground complex language-->
<!--						prompts onto an image with no segmentation specific training. An-->
<!--						off-the-shelf diffusion model is used with only an inference time-->
<!--						optimization technique to generate these segmentations.-->
						<b>Peekaboo</b> is a zero-shot segmentation algorithm, that takes in an image and a prompt to find the corresponding image region.
						This is, to our knowledge, the first open-vocabulary segmentation algorithm that is aware of pop references as well as more basic tasks. What's more - it doesn't need to be trained to do this, as it uses Stable Diffusion's weights off the shelf (without fine-tuning).
						In this paper, we show that diffusion based text-to-image models can perform zero-shot segmentation.
						Our implementation is publicly available.
					</div>
				</td>
			</tr>
		</table>
	</div>

	<hr>

<!--	Abstract-->
	<table align=center width=850px>
		<div style="text-align: center;"><h1>Abstract</h1></div>
		<tr>
			<td>
				<div style="text-align: justify;">
					Recent diffusion-based generative models combined with vision-language models are capable of creating
					realistic images from natural language prompts. While these models are trained on large internet-scale
					datasets, such pre-trained models are not directly introduced to any semantic localization or grounding.
					Most current approaches for localization or grounding rely on human-annotated localization information
					in the form of bounding boxes or segmentation masks. The exceptions are a few unsupervised methods that
					utilize architectures or loss functions geared towards localization, but they need to be trained 
					separately. In this work, we explore how off-the-shelf diffusion models, trained with no exposure to 
					such localization information, are capable of grounding various semantic phrases with no segmentation
					specific re-training. An inference time optimization process is introduced, that is capable of generating
					segmentation masks conditioned on natural language. We evaluate our proposal Peekaboo for unsupervised 
					semantic segmentation on the Pascal VOC dataset. In addition, we evaluate for referring segmentation on
					the RefCOCO dataset. In summary, we present a first zero-shot, open-vocabulary, unsupervised (no localization
					information), semantic grounding technique leveraging diffusion-based generative models with no re-training.
				</div>
			</td>
		</tr>
	</table>
	<br>
	<hr>

<!--	Methodology-->
	<table align=center width=850px>
		<div style="text-align: center;"><h1>Methodology</h1></div>
		<tr>

			<td width=700px>
				<div style="text-align: center;">
					<img class="round" style="width:700px" src="./resources/method.png"/>
				</div>
			</td>
		</tr>
		<tr>
			<td>
				<div style="text-align: justify;">
					<b>Overview of Peekaboo Architecture:</b> The image to be segmented is subject to alpha compositing with a learnable mask
					represented as an implicit neural image. The composite image and text prompt relating to the image region to be segmented are fed to
					our proposed dream loss, which is optimized iteratively. At the end of optimization, the implicit neural image converges to the
					optimal segmentation mask. We highlight that our dream loss is used only for learning a mask and not for any re-training of the diffusion
					model.
				</div>
			</td>
		</tr>
	</table>
	<br>


<!--	Citations-->
	<hr>
	<center><h1>Citation</h1></center>
	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/citation.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	<hr>


<!--	<table align=center width=900px>-->
<!--		<tr>-->
<!--			<td width=400px>-->
<!--				<left>-->
<!--					<center><h1>Acknowledgements</h1></center>-->
<!--					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.-->
<!--				</left>-->
<!--			</td>-->
<!--		</tr>-->
<!--	</table>-->

<br>
</body>
</html>

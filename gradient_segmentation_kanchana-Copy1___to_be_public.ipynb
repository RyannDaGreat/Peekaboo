{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5021e016-b2ef-4532-b2cf-2f7c1d7563e5",
   "metadata": {},
   "source": [
    "TODO: Set up an exclusive alpha map; so it has to choose between alphas for multi prompts on a single image. For example, a region that has \"jean luc picard\" cannot be the same as a region that has \"emma watson\". If we want we could relax or play with those constraints; but let's keep it simple first. We want to eventualy build a map of the whole image with all classes that way.\n",
    "\n",
    "To do that, add loss penalizing common alpha (dot product between offending alpha masks)\n",
    "\n",
    "If total alpha dips below some threshold, or some kinda statistic - it means it failed to find the thing\n",
    "\n",
    "To combine masks, we can have another optimization that allows weighted averages of all previously found masks until we get one we like.\n",
    "\n",
    "TODO: Average the results across multiple runs. What can we do with that? \n",
    "\n",
    "TODO: Add some kinda priors for shape; like superpixels or something. Instance segmentation can kinda do this, so can bilaterl blurs...\n",
    "\n",
    "TODO: Multiple classes - use stabledifusion prompt subtraction\n",
    "\n",
    "TODO: Get mean prompt across texts\n",
    "\n",
    "Ways to think about it:\n",
    "- What can we chip away while keeping the given prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7b35-00c7-4ec8-beb3-205a09d1f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rp\n",
    "import nerf.sd as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ryan.source.learnable_textures import LearnableTexturePackRaster,LearnableTexturePackFourier\n",
    "from ryan.source.learnable_textures import LearnableImageRaster,LearnableImageFourier, LearnableImageRasterBilateral, LearnableImageFourierBilateral\n",
    "import icecream\n",
    "from IPython.display import clear_output\n",
    "from easydict import EasyDict\n",
    "from ryan.bilateral_blur import BilateralProxyBlur\n",
    "import timm\n",
    "from torchvision.transforms.functional import normalize\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.vision_transformer import vit_base_patch16_224_dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ad4a1-e8b2-4db6-ba6e-cb4da32fb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    s=sd.StableDiffusion('cuda:2',\"CompVis/stable-diffusion-v1-4\")\n",
    "    # s=sd.StableDiffusion('cuda:0',\"/raid/xiangli/Codes/VOC-model/car\")\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ae188-d1b7-49e8-a76d-b38fcd2679a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTINGS\n",
    "def make_learnable_image(height, width, num_channels, foreground=None):\n",
    "    #Here we determine our image parametrization schema\n",
    "    bilateral_blur =  BilateralProxyBlur(foreground,**bilateral_kwargs)\n",
    "    return LearnableImageFourierBilateral(bilateral_blur,num_channels) #A neural neural image\n",
    "    return LearnableImageRasterBilateral(bilateral_blur,num_channels) #A neural neural image\n",
    "    return LearnableImageFourier(height,width,num_channels) #A neural neural image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b696c2-ed14-4833-86d2-7f85f491aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_torch_images(foreground, background, alpha):\n",
    "    #Input assertions\n",
    "    assert foreground.shape==background.shape\n",
    "    C,H,W=foreground.shape\n",
    "    assert alpha.shape==(H,W), 'alpha is a matrix'\n",
    "    \n",
    "    return foreground*alpha + background*(1-alpha)\n",
    "\n",
    "class PeekabooSegmenter(nn.Module):\n",
    "    def __init__(self, image:np.ndarray, labels:list, size:int=256, name:str='Untitled'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        height=width=size #We use square images for now\n",
    "        \n",
    "        assert all(issubclass(type(label),BaseLabel) for label in labels)\n",
    "        assert len(labels), 'Must have at least one class to segment'\n",
    "        \n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.labels=labels\n",
    "        self.name=name\n",
    "        \n",
    "        assert rp.is_image(image), 'Input should be a numpy image'\n",
    "        image=rp.cv_resize_image(image,(height,width))\n",
    "        image=rp.as_rgb_image(image) #Make sure it has 3 channels in HWC form\n",
    "        image=rp.as_float_image(image) #Make sure it's values are between 0 and 1\n",
    "        assert image.shape==(height,width,3) and image.min()>=0 and image.max()<=1\n",
    "        self.image=image\n",
    "        \n",
    "        self.foreground=rp.as_torch_image(image).to(device) #Convert the image to a torch tensor in CHW form\n",
    "        assert self.foreground.shape==(3, height, width)\n",
    "        \n",
    "        self.background=self.foreground*0 #The background will be a solid color for now\n",
    "        \n",
    "        self.alphas=make_learnable_image(height,width,num_channels=self.num_labels,foreground=self.foreground)\n",
    "            \n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.labels)\n",
    "            \n",
    "    def set_background_color(self, color):\n",
    "        r,g,b = color\n",
    "        assert 0<=r<=1 and 0<=g<=1 and 0<=b<=1\n",
    "        self.background[0]=r\n",
    "        self.background[1]=g\n",
    "        self.background[2]=b\n",
    "        \n",
    "    def randomize_background(self):\n",
    "        self.set_background_color(rp.random_rgb_float_color())\n",
    "        \n",
    "    def forward(self, alphas=None, return_alphas=False):\n",
    "        output_images = []\n",
    "        \n",
    "        if alphas is None:\n",
    "            alphas=self.alphas()\n",
    "        \n",
    "        assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "        assert alphas.min()>=0 and alphas.max()<=1\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            output_image=blend_torch_images(foreground=self.foreground, background=self.background, alpha=alpha)\n",
    "            output_images.append(output_image)\n",
    "            \n",
    "        output_images=torch.stack(output_images)\n",
    "        \n",
    "        assert output_images.shape==(self.num_labels, 3, self.height, self.width) #In BCHW form\n",
    "        \n",
    "        if return_alphas:\n",
    "            return output_images, alphas\n",
    "        else:\n",
    "            return output_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ace6f-1b42-44e9-ae82-0e3fd2813a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(self):\n",
    "    #This is a method of PeekabooSegmenter, but can be changed without rewriting the class if you want to change the display\n",
    "\n",
    "    colors = [(0,0,0), (1,1,1), ]#(1,0,0), (0,1,0), (0,0,1)] #Colors used to make the display\n",
    "    alphas = rp.as_numpy_array(self.alphas())\n",
    "    image = self.image\n",
    "    assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "\n",
    "    composites = []\n",
    "    for color in colors:\n",
    "        self.set_background_color(color)\n",
    "        column=rp.as_numpy_images(self(self.alphas()))\n",
    "        composites.append(column)\n",
    "\n",
    "    label_names=[label.name for label in self.labels]\n",
    "\n",
    "    stats_lines = [\n",
    "        self.name,\n",
    "        '',\n",
    "        'H,W = %ix%i'%(self.height,self.width),\n",
    "    ]\n",
    "\n",
    "    def try_add_stat(stat_format, var_name):\n",
    "        if var_name in globals():\n",
    "            stats_line=stat_format%globals()[var_name]\n",
    "            stats_lines.append(stats_line)\n",
    "\n",
    "    try_add_stat('Gravity: %.2e','GRAVITY'   )\n",
    "    try_add_stat('Batch Size: %i','BATCH_SIZE')\n",
    "    try_add_stat('Iter: %i','iter_num')\n",
    "    try_add_stat('Init Iters: %i','INIT_ITERS')\n",
    "    try_add_stat('Image Name: %s','image_filename')\n",
    "    try_add_stat('Learning Rate: %.2e','LEARNING_RATE')\n",
    "    try_add_stat('Guidance: %i%%','GUIDANCE_SCALE')\n",
    "\n",
    "    stats_image=rp.labeled_image(self.image, rp.line_join(stats_lines), \n",
    "                                 size=15*len(stats_lines), \n",
    "                                 position='bottom', align='center')\n",
    "\n",
    "    composite_grid=rp.grid_concatenated_images([\n",
    "        rp.labeled_images(alphas,label_names),\n",
    "        *composites\n",
    "    ])\n",
    "\n",
    "    output_image = rp.horizontally_concatenated_images(stats_image, composite_grid)\n",
    "\n",
    "    rp.display_image(output_image)\n",
    "\n",
    "    return output_image\n",
    "\n",
    "PeekabooSegmenter.display=display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f66e9-c2f3-463f-bb21-7010b80ecbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_embedding(prompts:list):\n",
    "    return torch.mean(\n",
    "        torch.stack(\n",
    "            [s.get_text_embeds(prompt) for prompt in prompts]\n",
    "        ),\n",
    "        dim=0\n",
    "    ).to(device)\n",
    "\n",
    "class BaseLabel:\n",
    "    def __init__(self, name:str, embedding:torch.Tensor):\n",
    "        #Later on we might have more sophisticated embeddings, such as averaging multiple prompts\n",
    "        #We also might have associated colors for visualization, or relations between labels\n",
    "        self.name=name\n",
    "        self.embedding=embedding\n",
    "        \n",
    "    def get_sample_image(self):\n",
    "        output=s.embed_to_img(self.embedding)[0]\n",
    "        assert rp.is_image(output)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s(name=%s)'%(type(self).__name__,self.name)\n",
    "        \n",
    "class SimpleLabel(BaseLabel):\n",
    "    def __init__(self, name:str):\n",
    "        super().__init__(name, s.get_text_embeds(name).to(device))\n",
    "\n",
    "class MeanLabel(BaseLabel):\n",
    "    #Test: rp.display_image(rp.horizontally_concatenated_images(MeanLabel('Dogcat','dog','cat').get_sample_image() for _ in range(1)))\n",
    "    def __init__(self, name:str, *prompts):\n",
    "        prompts=rp.detuple(prompts)\n",
    "        super().__init__(name, get_mean_embedding(prompts))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c2611-8944-4158-91ca-31b25b737dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_cell(cell_title):\n",
    "    rp.fansi_print(\"<Cell: %s>\"%cell_title, 'cyan', 'underlined')\n",
    "    rp.ptoc()\n",
    "def log(x):\n",
    "    x=str(x)\n",
    "    rp.fansi_print(x, 'yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b041c6-346e-435f-bdbc-c8b988491855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeekabooResults(EasyDict):\n",
    "    #Acts like a dict, except you can read/write parameters by doing self.thing instead of self['thing']\n",
    "    pass\n",
    "\n",
    "def save_peekaboo_results(results,new_folder_path):\n",
    "    assert not rp.folder_exists(new_folder_path), 'Please use a different name, not %s'%new_folder_path\n",
    "    rp.make_folder(new_folder_path)\n",
    "    with rp.SetCurrentDirectoryTemporarily(new_folder_path):\n",
    "        log(\"Saving PeekabooResults to \"+new_folder_path)\n",
    "        params={}\n",
    "        for key in results:\n",
    "            value=results[key]\n",
    "            if rp.is_image(value): \n",
    "                #Save a single image\n",
    "                rp.save_image(value,key+'.png')\n",
    "            elif isinstance(value, np.ndarray) and rp.is_image(value[0]):\n",
    "                #Save a folder of images\n",
    "                rp.make_directory(key)\n",
    "                with rp.SetCurrentDirectoryTemporarily(key):\n",
    "                    for i in range(len(value)):\n",
    "                        rp.save_image(value[i],str(i)+'.png')\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                #Save a generic numpy array\n",
    "                np.save(key+'.npy',value) \n",
    "            else:\n",
    "\n",
    "                import json\n",
    "                try:\n",
    "                    json.dumps({key:value})\n",
    "                    #Assume value is json-parseable\n",
    "                    params[key]=value\n",
    "                except Exception:\n",
    "                    params[key]=str(value)\n",
    "        rp.save_json(params,'params.json',pretty=True)\n",
    "        log(\"Done saving PeekabooResults to \"+new_folder_path+\"!\")\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ffff5-e294-4bef-9010-7dec22d3a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_peekaboo(label, image):\n",
    "    assert rp.is_image(image)\n",
    "    assert issubclass(type(label),BaseLabel)\n",
    "    image=rp.as_rgb_image(rp.as_float_image(image))\n",
    "    rp.tic()\n",
    "    time_started=rp.get_current_date()\n",
    "    \n",
    "    \n",
    "    log_cell('Get Hyperparameters') ########################################################################\n",
    "    global GRAVITY\n",
    "    global BATCH_SIZE\n",
    "    global NUM_ITER\n",
    "    global GUIDANCE_SCALE\n",
    "    global LEARNING_RATE\n",
    "    global INIT_ITERS\n",
    "    global bilateral_kwargs\n",
    "    icecream.ic(GRAVITY, BATCH_SIZE, NUM_ITER, GUIDANCE_SCALE, INIT_ITERS, bilateral_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Alpha Initializer') ########################################################################\n",
    "\n",
    "    p=PeekabooSegmenter(image,labels=[label], name=name).to(device)\n",
    "\n",
    "    blur_image=rp.as_numpy_image(p.alphas.bilateral_blur(p.foreground))\n",
    "    rp.display_image(blur_image)\n",
    "\n",
    "    p.display();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create Optimizers') ########################################################################\n",
    "\n",
    "    params=list(p.parameters())\n",
    "    optim=torch.optim.Adam(params,lr=1e-3)\n",
    "    optim=torch.optim.SGD(params,lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create DINO Map') ########################################################################\n",
    "\n",
    "    if \"dino_model\" not in dir():\n",
    "        dino_model = vit_base_patch16_224_dino(True)\n",
    "\n",
    "    dino_preview_image=None\n",
    "    def get_dino_map(image, contrast=4):\n",
    "        norm_image = rp.as_torch_image(rp.as_rgb_image(rp.as_float_image(rp.cv_resize_image(image, (224, 224)))))\n",
    "        norm_image = normalize(norm_image, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "\n",
    "        sample = norm_image[None]\n",
    "        assert sample.shape==(1, 3, 224, 224)\n",
    "\n",
    "        out = dino_model.forward_features(sample)\n",
    "\n",
    "        vis = out[0, 1:].reshape(14, 14, -1)\n",
    "\n",
    "        TOKEN_NUMBER = 0  # 0 is classification token, all others are spatial\n",
    "\n",
    "        vis = vis @ out[0, TOKEN_NUMBER]\n",
    "        vis = rp.full_range(vis)\n",
    "\n",
    "        dino_map = 1 - rp.cv_resize_image(\n",
    "            rp.as_numpy_array(vis), rp.get_image_dimensions(image)\n",
    "        )\n",
    "\n",
    "        dino_map = ((dino_map-1/2)*contrast)+1/2\n",
    "        dino_map = np.clip(dino_map, 0, 1)\n",
    "\n",
    "        nonlocal dino_preview_image\n",
    "        dino_preview_image=(\n",
    "            rp.horizontally_concatenated_images(\n",
    "                dino_map,\n",
    "                rp.blend_images(\n",
    "                    rp.blend_images(image, 0, (dino_map < 0.5) * 0.8),\n",
    "                    (0, 1, 0),\n",
    "                    rp.cv_dilate(rp.auto_canny(dino_map < 0.5),diameter=2),\n",
    "                ),\n",
    "            )\n",
    "        ) \n",
    "        rp.display_image(dino_preview_image)\n",
    "\n",
    "        assert dino_map.shape==rp.get_image_dimensions(image)\n",
    "\n",
    "        dino_map = torch.Tensor(dino_map[None,:,:]).to(device) #Make it a torch image with 1 channel\n",
    "\n",
    "        return dino_map\n",
    "\n",
    "    dino_map=get_dino_map(p.image)\n",
    "    icecream.ic(dino_map.shape);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('DINO Initialization Pretraining') ########################################################################\n",
    "\n",
    "    #Pre-train the alphas to be like the initial_alphas\n",
    "\n",
    "    if INIT_ITERS:\n",
    "        log(\"Initializing with DINO please wait...\")\n",
    "\n",
    "        initial_alphas=dino_map \n",
    "\n",
    "        initial_anim=[]\n",
    "        initial_losses=[]\n",
    "\n",
    "        display_eta=rp.eta(INIT_ITERS)\n",
    "        for _ in range(INIT_ITERS):\n",
    "            display_eta(_)\n",
    "\n",
    "            alphas=p.alphas()\n",
    "            loss = ((alphas - initial_alphas)**2).sum()\n",
    "            initial_losses.append(float(loss))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # if not _%(INIT_ITERS//10):\n",
    "            #     clear_output()\n",
    "            #     frame=rp.horizontally_concatenated_images(rp.as_numpy_array(alphas))\n",
    "            #     rp.display_image(frame)\n",
    "            #     initial_anim.append(frame)\n",
    "\n",
    "        # clear_output()\n",
    "        # rp.display_image_slideshow(initial_anim)\n",
    "        # rp.display_image(frame)\n",
    "        rp.line_graph_via_bokeh(initial_losses, title='Initialization Losses',xlabel='Iter',ylabel='Loss')\n",
    "    else:\n",
    "        log(\"Skipping DINO Initialization because INIT_ITERS=0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create Logs') ########################################################################\n",
    "    global iter_num\n",
    "    iter_num=0\n",
    "    timelapse_frames=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Do Training') ########################################################################\n",
    "    preview_interval=NUM_ITER//10 #Show 10 preview images throughout training to prevent output from being truncated\n",
    "    preview_interval=max(1,preview_interval)\n",
    "    log(\"Will show preview images every %i iterations\"%(preview_interval))\n",
    "\n",
    "    try:\n",
    "        display_eta=rp.eta(NUM_ITER)\n",
    "        for _ in range(NUM_ITER):\n",
    "            display_eta(_)\n",
    "            iter_num+=1\n",
    "\n",
    "            alphas=p.alphas()\n",
    "\n",
    "            for __ in range(BATCH_SIZE):\n",
    "                p.randomize_background()\n",
    "                composites=p()\n",
    "                for label, composite in zip(p.labels, composites):\n",
    "                    s.train_step(label.embedding, composite[None], \n",
    "                                 guidance_scale=GUIDANCE_SCALE\n",
    "                                )\n",
    "\n",
    "            ((alphas.sum())*GRAVITY).backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # if not _%100:\n",
    "                    #Don't overflow the notebook\n",
    "                    # clear_output()\n",
    "                if not _%preview_interval: \n",
    "                    timelapse_frames.append(p.display())\n",
    "                    rp.ptoc()\n",
    "    except KeyboardInterrupt:\n",
    "        log(\"Interrupted early, returning current results...\")\n",
    "        pass\n",
    "\n",
    "                \n",
    "    rp.ptoc()\n",
    "    return PeekabooResults(\n",
    "        #The main output is the alphas\n",
    "        alphas=rp.as_numpy_array(alphas),\n",
    "        \n",
    "        #Keep track of hyperparameters used\n",
    "        GRAVITY=GRAVITY,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        NUM_ITER=NUM_ITER,\n",
    "        GUIDANCE_SCALE=GUIDANCE_SCALE,\n",
    "        INIT_ITERS=INIT_ITERS,\n",
    "        bilateral_kwargs=bilateral_kwargs,\n",
    "        \n",
    "        #Keep track of the inputs used\n",
    "        label=label,\n",
    "        image=image,\n",
    "        \n",
    "        #Record some extra info\n",
    "        preview_image=p.display(),\n",
    "        timelapse_frames=rp.as_numpy_array(timelapse_frames),\n",
    "        dino_preview_image=dino_preview_image,\n",
    "        blur_image=blur_image,\n",
    "        height=p.height,\n",
    "        width=p.width,\n",
    "        p_name=p.name,\n",
    "        \n",
    "        git_hash=rp.get_current_git_hash(), \n",
    "        time_started=rp.r._format_datetime(time_started),\n",
    "        time_completed=rp.r._format_datetime(rp.get_current_date()),\n",
    "        device=device,\n",
    "        computer_name=rp.get_computer_name(),\n",
    "    ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1220d39-60d1-4d09-9d6c-e8aa7d4072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient(label, image):\n",
    "    assert rp.is_image(image)\n",
    "    assert issubclass(type(label),BaseLabel)\n",
    "    image=rp.as_rgb_image(rp.as_float_image(image))\n",
    "    rp.tic()\n",
    "    time_started=rp.get_current_date()\n",
    "    \n",
    "    \n",
    "    log_cell('Get Hyperparameters') ########################################################################\n",
    "    global GRAVITY\n",
    "    global BATCH_SIZE\n",
    "    global NUM_ITER\n",
    "    global GUIDANCE_SCALE\n",
    "    global LEARNING_RATE\n",
    "    global INIT_ITERS\n",
    "    global bilateral_kwargs\n",
    "    icecream.ic(GRAVITY, BATCH_SIZE, NUM_ITER, GUIDANCE_SCALE, INIT_ITERS, bilateral_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Alpha Initializer') ########################################################################\n",
    "\n",
    "    p=PeekabooSegmenter(image,labels=[label], name=name).to(device)\n",
    "\n",
    "    blur_image=rp.as_numpy_image(p.alphas.bilateral_blur(p.foreground))\n",
    "    rp.display_image(blur_image)\n",
    "\n",
    "    p.display();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create Optimizers') ########################################################################\n",
    "\n",
    "    params=list(p.parameters())\n",
    "    optim=torch.optim.Adam(params,lr=1e-3)\n",
    "    optim=torch.optim.SGD(params,lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create DINO Map') ########################################################################\n",
    "\n",
    "    if \"dino_model\" not in dir():\n",
    "        dino_model = vit_base_patch16_224_dino(True)\n",
    "\n",
    "    dino_preview_image=None\n",
    "    def get_dino_map(image, contrast=4):\n",
    "        norm_image = rp.as_torch_image(rp.as_rgb_image(rp.as_float_image(rp.cv_resize_image(image, (224, 224)))))\n",
    "        norm_image = normalize(norm_image, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "\n",
    "        sample = norm_image[None]\n",
    "        assert sample.shape==(1, 3, 224, 224)\n",
    "\n",
    "        out = dino_model.forward_features(sample)\n",
    "\n",
    "        vis = out[0, 1:].reshape(14, 14, -1)\n",
    "\n",
    "        TOKEN_NUMBER = 0  # 0 is classification token, all others are spatial\n",
    "\n",
    "        vis = vis @ out[0, TOKEN_NUMBER]\n",
    "        vis = rp.full_range(vis)\n",
    "\n",
    "        dino_map = 1 - rp.cv_resize_image(\n",
    "            rp.as_numpy_array(vis), rp.get_image_dimensions(image)\n",
    "        )\n",
    "\n",
    "        dino_map = ((dino_map-1/2)*contrast)+1/2\n",
    "        dino_map = np.clip(dino_map, 0, 1)\n",
    "\n",
    "        nonlocal dino_preview_image\n",
    "        dino_preview_image=(\n",
    "            rp.horizontally_concatenated_images(\n",
    "                dino_map,\n",
    "                rp.blend_images(\n",
    "                    rp.blend_images(image, 0, (dino_map < 0.5) * 0.8),\n",
    "                    (0, 1, 0),\n",
    "                    rp.cv_dilate(rp.auto_canny(dino_map < 0.5),diameter=2),\n",
    "                ),\n",
    "            )\n",
    "        ) \n",
    "        rp.display_image(dino_preview_image)\n",
    "\n",
    "        assert dino_map.shape==rp.get_image_dimensions(image)\n",
    "\n",
    "        dino_map = torch.Tensor(dino_map[None,:,:]).to(device) #Make it a torch image with 1 channel\n",
    "\n",
    "        return dino_map\n",
    "\n",
    "    dino_map=get_dino_map(p.image)\n",
    "    icecream.ic(dino_map.shape);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('DINO Initialization Pretraining') ########################################################################\n",
    "\n",
    "    #Pre-train the alphas to be like the initial_alphas\n",
    "\n",
    "    if INIT_ITERS:\n",
    "        log(\"Initializing with DINO please wait...\")\n",
    "\n",
    "        initial_alphas=dino_map \n",
    "\n",
    "        initial_anim=[]\n",
    "        initial_losses=[]\n",
    "\n",
    "        display_eta=rp.eta(INIT_ITERS)\n",
    "        for _ in range(INIT_ITERS):\n",
    "            display_eta(_)\n",
    "\n",
    "            alphas=p.alphas()\n",
    "            loss = ((alphas - initial_alphas)**2).sum()\n",
    "            initial_losses.append(float(loss))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            # if not _%(INIT_ITERS//10):\n",
    "            #     clear_output()\n",
    "            #     frame=rp.horizontally_concatenated_images(rp.as_numpy_array(alphas))\n",
    "            #     rp.display_image(frame)\n",
    "            #     initial_anim.append(frame)\n",
    "\n",
    "        # clear_output()\n",
    "        # rp.display_image_slideshow(initial_anim)\n",
    "        # rp.display_image(frame)\n",
    "        rp.line_graph_via_bokeh(initial_losses, title='Initialization Losses',xlabel='Iter',ylabel='Loss')\n",
    "    else:\n",
    "        log(\"Skipping DINO Initialization because INIT_ITERS=0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Create Logs') ########################################################################\n",
    "    global iter_num\n",
    "    iter_num=0\n",
    "    timelapse_frames=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    log_cell('Do Training') ########################################################################\n",
    "    preview_interval=NUM_ITER//10 #Show 10 preview images throughout training to prevent output from being truncated\n",
    "    preview_interval=max(1,preview_interval)\n",
    "    log(\"Will show preview images every %i iterations\"%(preview_interval))\n",
    "\n",
    "    try:\n",
    "        display_eta=rp.eta(NUM_ITER)\n",
    "        for _ in range(NUM_ITER):\n",
    "            display_eta(_)\n",
    "            iter_num+=1\n",
    "\n",
    "            alphas=p.alphas()\n",
    "\n",
    "            for __ in range(BATCH_SIZE):\n",
    "                p.randomize_background()\n",
    "                composites=p()\n",
    "                for label, composite in zip(p.labels, composites):\n",
    "                    s.train_step(label.embedding, composite[None], \n",
    "                                 guidance_scale=GUIDANCE_SCALE\n",
    "                                )\n",
    "\n",
    "            ((alphas.sum())*GRAVITY).backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # if not _%100:\n",
    "                    #Don't overflow the notebook\n",
    "                    # clear_output()\n",
    "                if not _%preview_interval: \n",
    "                    timelapse_frames.append(p.display())\n",
    "                    rp.ptoc()\n",
    "    except KeyboardInterrupt:\n",
    "        log(\"Interrupted early, returning current results...\")\n",
    "        pass\n",
    "\n",
    "                \n",
    "    rp.ptoc()\n",
    "    return PeekabooResults(\n",
    "        #The main output is the alphas\n",
    "        alphas=rp.as_numpy_array(alphas),\n",
    "        \n",
    "        #Keep track of hyperparameters used\n",
    "        GRAVITY=GRAVITY,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        NUM_ITER=NUM_ITER,\n",
    "        GUIDANCE_SCALE=GUIDANCE_SCALE,\n",
    "        INIT_ITERS=INIT_ITERS,\n",
    "        bilateral_kwargs=bilateral_kwargs,\n",
    "        \n",
    "        #Keep track of the inputs used\n",
    "        label=label,\n",
    "        image=image,\n",
    "        \n",
    "        #Record some extra info\n",
    "        preview_image=p.display(),\n",
    "        timelapse_frames=rp.as_numpy_array(timelapse_frames),\n",
    "        dino_preview_image=dino_preview_image,\n",
    "        blur_image=blur_image,\n",
    "        height=p.height,\n",
    "        width=p.width,\n",
    "        p_name=p.name,\n",
    "        \n",
    "        git_hash=rp.get_current_git_hash(), \n",
    "        time_started=rp.r._format_datetime(time_started),\n",
    "        time_completed=rp.r._format_datetime(rp.get_current_date()),\n",
    "        device=device,\n",
    "        computer_name=rp.get_computer_name(),\n",
    "    ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e452b72-d080-4fa2-8a32-3647bd126ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xiang's Dataloader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "voc_label_map = {\n",
    "    0: 'background',\n",
    "    1: 'aeroplane',\n",
    "    2: 'bicycle',\n",
    "    3: 'bird',\n",
    "    4: 'boat',\n",
    "    5: 'bottle',\n",
    "    6: 'bus',\n",
    "    7: 'car',\n",
    "    8: 'cat',\n",
    "    9: 'chair',\n",
    "    10: 'cow',\n",
    "    11: 'dining table',\n",
    "    12: 'dog',\n",
    "    13: 'horse',\n",
    "    14: 'motorcycle',\n",
    "    # 14: 'motorbike',\n",
    "    # 15: 'person',\n",
    "    15: 'man',\n",
    "    16: 'potted plant',\n",
    "    17: 'sheep',\n",
    "    18: 'sofa',\n",
    "    19: 'train',\n",
    "    # 20: 'television monitor',\n",
    "    20: 'monitor',\n",
    "}\n",
    "\n",
    "voc_prompts={}\n",
    "for x in voc_label_map:\n",
    "    voc_prompts[x]='a '+voc_label_map[x]\n",
    "\n",
    "\n",
    "class SegDataset:\n",
    "    pass\n",
    "\n",
    "\n",
    "class CroppedPascalDataset(SegDataset):\n",
    "\n",
    "    def __init__(self, root, split=\"val\"):\n",
    "        self.root = root\n",
    "        # assert split in [\"train\", \"val\"], f\"invalid split: {split}\"\n",
    "        self.split = split\n",
    "        anno_path = f\"{root}/ImageSets/Segmentation/{split}.txt\"\n",
    "        self.file_list = self.get_file_list(anno_path)\n",
    "        self.label_remap = voc_label_map\n",
    "\n",
    "    def get_file_list(self, path):\n",
    "        with open(path, \"r\") as fo:\n",
    "            files = fo.readlines()\n",
    "            # print(files[0], f\"{self.root}/CroppedImages/cropped-{files[0].strip()}.jpg\")\n",
    "        files = [x.strip() for x in files if os.path.exists(f\"{self.root}/CroppedImages/cropped-{x.strip()}.jpg\")]\n",
    "        # files=rp.shuffled(files)\n",
    "        print(f'Found {len(files)} files.')\n",
    "        return files\n",
    "\n",
    "    @staticmethod\n",
    "    def load_image(path, get_arr=True):\n",
    "        if get_arr:\n",
    "            return np.array(Image.open(path))\n",
    "        else:\n",
    "            return Image.open(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, item, get_arr=True):\n",
    "        if isinstance(item, int):\n",
    "            item = item % len(self.file_list)\n",
    "            cur_idx = self.file_list[item]\n",
    "        elif isinstance(item, str):\n",
    "            cur_idx = item\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Invalid item dtype: {type(item)}\")\n",
    "        img_path = f\"{self.root}/CroppedImages/cropped-{cur_idx}.jpg\"\n",
    "        seg_path = f\"{self.root}/SegmentationClass/{cur_idx}.png\"\n",
    "\n",
    "        img = self.load_image(img_path, get_arr=get_arr)\n",
    "        seg = self.load_image(seg_path, get_arr=get_arr)\n",
    "        \n",
    "        cropped_path = f\"{self.root}/CroppedImages/cropped-{cur_idx}.jpg.txt\"\n",
    "        # print(seg.shape)\n",
    "        with open(cropped_path, 'r') as f:\n",
    "            info = f.readline().split()\n",
    "            x1 = int(info[0])\n",
    "            x2 = int(info[1])\n",
    "            y1 = int(info[2])\n",
    "            y2 = int(info[3])\n",
    "            # print(info)\n",
    "        seg = seg[y1:y2, x1:x2]\n",
    "        # print(img.shape, seg.shape)\n",
    "        assert seg.shape == img.shape[:2]\n",
    "        \n",
    "        seg_labels = np.unique(seg)\n",
    "        seg_labels = set(seg_labels) - {0, 255}\n",
    "\n",
    "        return EasyDict(\n",
    "            img=img,\n",
    "            seg=seg,\n",
    "            seg_labels=seg_labels,\n",
    "            cur_idx=cur_idx,\n",
    "            img_path=img_path,\n",
    "            seg_path=seg_path, \n",
    "            cropped_path=cropped_path,\n",
    "            names=[voc_label_map[x] for x in seg_labels],\n",
    "            prompts=[voc_prompts[x] for x in seg_labels],\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = \"/raid/datasets/pascal_voc/VOC2012\"\n",
    "    ds = CroppedPascalDataset(data_dir, split='seg0')\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    l_set = defaultdict(int)\n",
    "    for i in range(50):\n",
    "        for j in ds[i].seg_labels:\n",
    "            l_set[j] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d02d40e-25d7-473b-8a89-5bdcc088f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(a,b):\n",
    "    assert rp.is_image(a)\n",
    "    assert rp.is_image(b)\n",
    "    a=rp.as_grayscale_image(rp.as_float_image(a))\n",
    "    b=rp.as_grayscale_image(rp.as_float_image(b))\n",
    "    intersection=a*b\n",
    "    union=a+b-intersection\n",
    "    return intersection.sum()/union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b03f6-0468-41b3-b40e-36f59ab5508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the hyperparamers here as global variables\n",
    "GRAVITY=1e-2\n",
    "GRAVITY=1e-1/2\n",
    "# GRAVITY=1e-1\n",
    "# GRAVITY=1e-1*1.5\n",
    "# GRAVITY=1e-1*1.5\n",
    "BATCH_SIZE=1\n",
    "NUM_ITER=300\n",
    "NUM_ITER=300\n",
    "# NUM_ITER=3\n",
    "GUIDANCE_SCALE=100 #100 is default\n",
    "INIT_ITERS=50 #DINO Pretraining Iters\n",
    "INIT_ITERS=0\n",
    "LEARNING_RATE=1e-5\n",
    "bilateral_kwargs=dict(kernel_size = 3,\n",
    "                      # tolerance = .1,\n",
    "                      tolerance = .08,\n",
    "                      sigma = 5,\n",
    "                      iterations=40,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c75b8c-2598-4503-a237-60b8502d9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "I=0\n",
    "for x in ds:\n",
    "    import random\n",
    "    random.seed(31)\n",
    "    x=ds[rp.random_index(len(ds))]\n",
    "    \n",
    "    for name,prompt in zip(x.names,x.prompts):\n",
    "        name=prompt='an old red mercedes benz car, vintage car'\n",
    "        name=prompt='harry styles'\n",
    "        name=prompt='danny devito'\n",
    "        name=prompt='Arnold Schwarzenegger'\n",
    "        name=prompt='The Harry Potter Boy'\n",
    "        name=prompt='Harry Potter'\n",
    "        name=prompt='minecraft steve'\n",
    "        name=prompt='wall-e'\n",
    "        name=prompt='white eve robot'\n",
    "        name=prompt='c3po'\n",
    "        name=prompt='rubiks cube'\n",
    "        \n",
    "        name=prompt=['yellow rubber duck','blue rubber duck','orange rubber duck','green rubber duck','pink rubber duck'];\n",
    "        name=prompt=['red rubber duck',];\n",
    "        name=prompt=['a can of mtn dew', 'a can of sprite', 'a can of dr pepper', 'dr pepper', 'sprite', 'mtn dew', 'orange', 'yellow soda', 'orange soda', 'sunkist orange', 'sunkist yellow']\n",
    "        name=prompt=['strawberry jam jar']#,'skippy peanut butter']\n",
    "        name=prompt=['makise kurisu anime girl']#,'skippy peanut butter']\n",
    "        name=prompt=['Rintarou Okabe anime boy']#,'skippy peanut butter']\n",
    "        name=prompt=['Beverly Crusher']#,'skippy peanut butter']\n",
    "        name=prompt=['Jean Luc Picard']#,'skippy peanut butter']\n",
    "        name=prompt=[\"Gyarados\"]\n",
    "        name=prompt=[\"A cat\"]\n",
    "        name=prompt=[\"Strawberry ice cream cone\"]\n",
    "        name=prompt=prompt[I%len(prompt)]\n",
    "        I+=1\n",
    "        \n",
    "        \n",
    "        output_folder='peekaboo_results_kr/%i.%s.%s'%(rp.millis(),x.cur_idx,name)\n",
    "        print('NEW TEST!',output_folder)\n",
    "        icecream.ic(output_folder,name,prompt,x.cur_idx,x.img_path,x.seg_path,x.cropped_path,x.names,x.prompts)\n",
    "        label=SimpleLabel(prompt)\n",
    "        # results=run_peekaboo(label,x.img)\n",
    "        \n",
    "        # img=rp.load_image(\"https://www.padoniavets.com/sites/default/files/field/image/cats-and-dogs.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://img.etimg.com/thumb/msid-79778298,width-650,imgsize-1290800,,resizemode-4,quality-100/sales-of-classic-cars-have-remained-positively-stable-in-2020-.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://s2.r29static.com/bin/entry/bc7/0,46,460,460/1200x1200,80/1333127/image.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://s.yimg.com/ny/api/res/1.2/HIg4XSC.pN9vPMx_MHTv5w--/YXBwaWQ9aGlnaGxhbmRlcjt3PTY0MA--/https://s.yimg.com/os/creatr-uploaded-images/2021-10/69705cc0-2ac8-11ec-973b-482c29ca5c68\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://assets.teenvogue.com/photos/569e7d2a74da98670ff0ce1c/1:1/w_2159,h_2159,c_limit/MCDHAPO_EC797_H.JPG\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://www.nme.com/wp-content/uploads/2021/08/pokemon_brilliant_diamond_and_shining_pearl_starters_turtwig_chimchar_piplup.jpeg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://images.genius.com/ce44822bee903c5e777c004c8bcaa1ef.300x300x1.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://w0.peakpx.com/wallpaper/936/113/HD-wallpaper-wall-e-and-eve-and-e-wall-eve.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://www.irishtimes.com/resizer/q5bI2nij6BAIDT8AZeKKpyvm8T4=/1600x1200/filters:format(jpg):quality(70)/cloudfront-eu-central-1.images.arcpublishing.com/irishtimes/UDNDFXTEMKENA5YILPWQMHCKXU.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"ducks.png\",use_cache=True)\n",
    "        # img=rp.load_image(\"five_sodas.png\",use_cache=True)\n",
    "        # img=rp.load_image(\"jam_jelly_beans.png\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://64.media.tumblr.com/909bb24eae0b533481627a2963528da7/5fa507f8401885d2-f2/s540x810/48b07eb60b44467ddfd4892b616ec8d6812ddea4.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://www.slashfilm.com/img/gallery/star-trek-picard-season-3-features-the-best-material-yet-for-dr-crusher-according-to-gates-mcfadden-comic-con/crushers-role-in-star-trek-1658612419.jpg\",use_cache=True)\n",
    "        # img=rp.load_image(\"https://akns-images.eonline.com/eol_images/Entire_Site/2016917/rs_600x600-161017135949-600.finding-dory-2.101716.jpg\")\n",
    "        # img=rp.load_image(\"https://m.media-amazon.com/images/I/519l1cA-sTL.jpg\")\n",
    "        img=rp.load_image(\"https://www.shutterstock.com/image-photo/dog-cat-under-plaid-pet-260nw-726710023.jpg\")\n",
    "        img=rp.load_image(\"ice-cream-1.jpg\")\n",
    "        # img=rp.load_image(\"https://ima/ges-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/bbfdfeed-2118-4cd3-873b-279837423fa9/dd8x8v4-62e42076-34e3-42ce-94c5-01547f4a7239.png?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcL2JiZmRmZWVkLTIxMTgtNGNkMy04NzNiLTI3OTgzNzQyM2ZhOVwvZGQ4eDh2NC02MmU0MjA3Ni0zNGUzLTQyY2UtOTRjNS0wMTU0N2Y0YTcyMzkucG5nIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.I5vSY99f6m8YOAdtCj6sk6ZbLtDIe-IIWwfhNswI678\",use_cache=True)\n",
    "        # img=rp.crop_image(img,width=rp.get_image_height(img),origin='center')\n",
    "        \n",
    "        x.prompts[0]=prompt\n",
    "        \n",
    "        \n",
    "        results=run_peekaboo(SimpleLabel(prompt),img)\n",
    "        \n",
    "        \n",
    "        #Add extra data to results\n",
    "        results.x_prompt=prompt\n",
    "        results.x_name=name\n",
    "        # \n",
    "        results.x_seg = x.seg\n",
    "        results.x_seg_labels = x.seg_labels\n",
    "        results.x_cur_idx = x.cur_idx\n",
    "        results.x_img_path = x.img_path\n",
    "        results.x_seg_path = x.seg_path\n",
    "        results.x_cropped_path = x.cropped_path\n",
    "        results.x_names = x.names\n",
    "        results.x_prompts = x.prompts\n",
    "        \n",
    "        _alpha=results.alphas[0]\n",
    "        _seg=x.seg!=0\n",
    "        _seg=rp.cv_resize_image(_seg,(results.height,results.width))\n",
    "        results.IOU_Continuous=IOU(_alpha, _seg)\n",
    "        results['IOU_>.9']=IOU(_alpha>.9, _seg)\n",
    "        results['IOU_>.8']=IOU(_alpha>.8, _seg)\n",
    "        results['IOU_>.7']=IOU(_alpha>.7, _seg)\n",
    "        results['IOU_>.6']=IOU(_alpha>.6, _seg)\n",
    "        results['IOU_>.5']=IOU(_alpha>.5, _seg)\n",
    "        results['IOU_>.4']=IOU(_alpha>.4, _seg)\n",
    "        results['IOU_>.3']=IOU(_alpha>.3, _seg)\n",
    "        results['IOU_>.2']=IOU(_alpha>.2, _seg)\n",
    "        results['IOU_>.1']=IOU(_alpha>.1, _seg)\n",
    "        \n",
    "        \n",
    "                    \n",
    "        save_peekaboo_results(results,output_folder)\n",
    "        \n",
    "        from IPython.display import clear_output\n",
    "        \n",
    "        1/0\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed9c0b-dad9-4d5a-8f2b-582f692de7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://becs-table.com.au/wp-content/uploads/2014/01/ice-cream-1.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c736464-3ab3-432f-9274-06ce062f050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=rp.random_element(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d21f9-703c-4afb-846c-a7caf236103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set the hyperparamers here as global variables\n",
    "GRAVITY=1e-2\n",
    "# GRAVITY=1e-1\n",
    "BATCH_SIZE=2\n",
    "NUM_ITER=300\n",
    "NUM_ITER=500\n",
    "# NUM_ITER=3\n",
    "GUIDANCE_SCALE=50 #100 is default\n",
    "INIT_ITERS=50 #DINO Pretraining Iters\n",
    "INIT_ITERS=20\n",
    "INIT_ITERS=0\n",
    "LEARNING_RATE=1e-5\n",
    "bilateral_kwargs=dict(kernel_size = 3,\n",
    "                      tolerance = .1,\n",
    "                      # tolerance = .08,\n",
    "                      sigma = 5,\n",
    "                      iterations=40,\n",
    "                     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

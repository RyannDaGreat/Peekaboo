{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec7b35-00c7-4ec8-beb3-205a09d1f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "import icecream\n",
    "import numpy as np\n",
    "import rp\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from easydict import EasyDict\n",
    "from IPython.display import clear_output\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.vision_transformer import vit_base_patch16_224_dino\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "import source.stable_diffusion as sd\n",
    "from source.bilateral_blur import BilateralProxyBlur\n",
    "from source.learnable_textures import (LearnableImageFourier,\n",
    "                                       LearnableImageFourierBilateral,\n",
    "                                       LearnableImageRaster,\n",
    "                                       LearnableImageRasterBilateral,\n",
    "                                       LearnableTexturePackFourier,\n",
    "                                       LearnableTexturePackRaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ad4a1-e8b2-4db6-ba6e-cb4da32fb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    s=sd.StableDiffusion('cuda','CompVis/stable-diffusion-v1-4')\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d66702-a66e-4cb4-a25a-0522099f9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_learnable_image(height, width, num_channels, foreground=None, bilateral_kwargs:dict={}):\n",
    "    #Here we determine our image parametrization schema\n",
    "    bilateral_blur =  BilateralProxyBlur(foreground,**bilateral_kwargs)\n",
    "    return LearnableImageFourierBilateral(bilateral_blur,num_channels) #A neural neural image\n",
    "    return LearnableImageRasterBilateral(bilateral_blur,num_channels) #A neural neural image\n",
    "    return LearnableImageFourier(height,width,num_channels) #A neural neural image\n",
    "\n",
    "def blend_torch_images(foreground, background, alpha):\n",
    "    #Input assertions\n",
    "    assert foreground.shape==background.shape\n",
    "    C,H,W=foreground.shape\n",
    "    assert alpha.shape==(H,W), 'alpha is a matrix'\n",
    "    \n",
    "    return foreground*alpha + background*(1-alpha)\n",
    "\n",
    "class PeekabooSegmenter(nn.Module):\n",
    "    def __init__(self, image:np.ndarray, labels:List[BaseLabel], size:int=256, name:str='Untitled', bilateral_kwargs:dict={}):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        height=width=size #We use square images for now\n",
    "        \n",
    "        assert all(issubclass(type(label),BaseLabel) for label in labels)\n",
    "        assert len(labels), 'Must have at least one class to segment'\n",
    "        \n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.labels=labels\n",
    "        self.name=name\n",
    "        \n",
    "        assert rp.is_image(image), 'Input should be a numpy image'\n",
    "        image=rp.cv_resize_image(image,(height,width))\n",
    "        image=rp.as_rgb_image(image) #Make sure it has 3 channels in HWC form\n",
    "        image=rp.as_float_image(image) #Make sure it's values are between 0 and 1\n",
    "        assert image.shape==(height,width,3) and image.min()>=0 and image.max()<=1\n",
    "        self.image=image\n",
    "        \n",
    "        self.foreground=rp.as_torch_image(image).to(device) #Convert the image to a torch tensor in CHW form\n",
    "        assert self.foreground.shape==(3, height, width)\n",
    "        \n",
    "        self.background=self.foreground*0 #The background will be a solid color for now\n",
    "        \n",
    "        self.alphas=make_learnable_image(height,width,num_channels=self.num_labels,foreground=self.foreground)\n",
    "            \n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.labels)\n",
    "            \n",
    "    def set_background_color(self, color):\n",
    "        r,g,b = color\n",
    "        assert 0<=r<=1 and 0<=g<=1 and 0<=b<=1\n",
    "        self.background[0]=r\n",
    "        self.background[1]=g\n",
    "        self.background[2]=b\n",
    "        \n",
    "    def randomize_background(self):\n",
    "        self.set_background_color(rp.random_rgb_float_color())\n",
    "        \n",
    "    def forward(self, alphas=None, return_alphas=False):\n",
    "        output_images = []\n",
    "        \n",
    "        if alphas is None:\n",
    "            alphas=self.alphas()\n",
    "        \n",
    "        assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "        assert alphas.min()>=0 and alphas.max()<=1\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            output_image=blend_torch_images(foreground=self.foreground, background=self.background, alpha=alpha)\n",
    "            output_images.append(output_image)\n",
    "            \n",
    "        output_images=torch.stack(output_images)\n",
    "        \n",
    "        assert output_images.shape==(self.num_labels, 3, self.height, self.width) #In BCHW form\n",
    "        \n",
    "        if return_alphas:\n",
    "            return output_images, alphas\n",
    "        else:\n",
    "            return output_images\n",
    "\n",
    "def display(self):\n",
    "    #This is a method of PeekabooSegmenter, but can be changed without rewriting the class if you want to change the display\n",
    "\n",
    "    colors = [(0,0,0), (1,1,1), ]#(1,0,0), (0,1,0), (0,0,1)] #Colors used to make the display\n",
    "    alphas = rp.as_numpy_array(self.alphas())\n",
    "    image = self.image\n",
    "    assert alphas.shape==(self.num_labels, self.height, self.width)\n",
    "\n",
    "    composites = []\n",
    "    for color in colors:\n",
    "        self.set_background_color(color)\n",
    "        column=rp.as_numpy_images(self(self.alphas()))\n",
    "        composites.append(column)\n",
    "\n",
    "    label_names=[label.name for label in self.labels]\n",
    "\n",
    "    stats_lines = [\n",
    "        self.name,\n",
    "        '',\n",
    "        'H,W = %ix%i'%(self.height,self.width),\n",
    "    ]\n",
    "\n",
    "    def try_add_stat(stat_format, var_name):\n",
    "        if var_name in globals():\n",
    "            stats_line=stat_format%globals()[var_name]\n",
    "            stats_lines.append(stats_line)\n",
    "\n",
    "    try_add_stat('Gravity: %.2e','GRAVITY'   )\n",
    "    try_add_stat('Batch Size: %i','BATCH_SIZE')\n",
    "    try_add_stat('Iter: %i','iter_num')\n",
    "    try_add_stat('Image Name: %s','image_filename')\n",
    "    try_add_stat('Learning Rate: %.2e','LEARNING_RATE')\n",
    "    try_add_stat('Guidance: %i%%','GUIDANCE_SCALE')\n",
    "\n",
    "    stats_image=rp.labeled_image(self.image, rp.line_join(stats_lines), \n",
    "                                 size=15*len(stats_lines), \n",
    "                                 position='bottom', align='center')\n",
    "\n",
    "    composite_grid=rp.grid_concatenated_images([\n",
    "        rp.labeled_images(alphas,label_names),\n",
    "        *composites\n",
    "    ])\n",
    "\n",
    "    output_image = rp.horizontally_concatenated_images(stats_image, composite_grid)\n",
    "\n",
    "    rp.display_image(output_image)\n",
    "\n",
    "    return output_image\n",
    "\n",
    "PeekabooSegmenter.display=display\n",
    "\n",
    "def get_mean_embedding(prompts:list):\n",
    "    return torch.mean(\n",
    "        torch.stack(\n",
    "            [s.get_text_embeddings(prompt) for prompt in prompts]\n",
    "        ),\n",
    "        dim=0\n",
    "    ).to(device)\n",
    "\n",
    "class BaseLabel:\n",
    "    def __init__(self, name:str, embedding:torch.Tensor):\n",
    "        #Later on we might have more sophisticated embeddings, such as averaging multiple prompts\n",
    "        #We also might have associated colors for visualization, or relations between labels\n",
    "        self.name=name\n",
    "        self.embedding=embedding\n",
    "        \n",
    "    def get_sample_image(self):\n",
    "        output=s.embeddings_to_imgs(self.embedding)[0]\n",
    "        assert rp.is_image(output)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '%s(name=%s)'%(type(self).__name__,self.name)\n",
    "        \n",
    "class SimpleLabel(BaseLabel):\n",
    "    def __init__(self, name:str):\n",
    "        super().__init__(name, s.get_text_embeddings(name).to(device))\n",
    "\n",
    "class MeanLabel(BaseLabel):\n",
    "    #Test: rp.display_image(rp.horizontally_concatenated_images(MeanLabel('Dogcat','dog','cat').get_sample_image() for _ in range(1)))\n",
    "    def __init__(self, name:str, *prompts):\n",
    "        prompts=rp.detuple(prompts)\n",
    "        super().__init__(name, get_mean_embedding(prompts))\n",
    "    \n",
    "\n",
    "def log_cell(cell_title):\n",
    "    rp.fansi_print(\"<Cell: %s>\"%cell_title, 'cyan', 'underlined')\n",
    "    # rp.ptoc()\n",
    "def log(x):\n",
    "    x=str(x)\n",
    "    rp.fansi_print(x, 'yellow')\n",
    "\n",
    "class PeekabooResults(EasyDict):\n",
    "    #Acts like a dict, except you can read/write parameters by doing self.thing instead of self['thing']\n",
    "    pass\n",
    "\n",
    "def save_peekaboo_results(results,new_folder_path):\n",
    "    assert not rp.folder_exists(new_folder_path), 'Please use a different name, not %s'%new_folder_path\n",
    "    rp.make_folder(new_folder_path)\n",
    "    with rp.SetCurrentDirectoryTemporarily(new_folder_path):\n",
    "        log(\"Saving PeekabooResults to \"+new_folder_path)\n",
    "        params={}\n",
    "        for key in results:\n",
    "            value=results[key]\n",
    "            if rp.is_image(value): \n",
    "                #Save a single image\n",
    "                rp.save_image(value,key+'.png')\n",
    "            elif isinstance(value, np.ndarray) and rp.is_image(value[0]):\n",
    "                #Save a folder of images\n",
    "                rp.make_directory(key)\n",
    "                with rp.SetCurrentDirectoryTemporarily(key):\n",
    "                    for i in range(len(value)):\n",
    "                        rp.save_image(value[i],str(i)+'.png')\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                #Save a generic numpy array\n",
    "                np.save(key+'.npy',value) \n",
    "            else:\n",
    "\n",
    "                import json\n",
    "                try:\n",
    "                    json.dumps({key:value})\n",
    "                    #Assume value is json-parseable\n",
    "                    params[key]=value\n",
    "                except Exception:\n",
    "                    params[key]=str(value)\n",
    "        rp.save_json(params,'params.json',pretty=True)\n",
    "        log(\"Done saving PeekabooResults to \"+new_folder_path+\"!\")\n",
    "        \n",
    "def make_image_square(image:np.ndarray, method='crop')->np.ndarray:\n",
    "    #Takes any image and makes it into a 512x512 square image with shape (512,512,3)\n",
    "    assert rp.is_image(image)\n",
    "    assert method in ['crop','scale']\n",
    "    image=rp.as_rgb_image(image)\n",
    "    \n",
    "    height, width = rp.get_image_dimensions(image)\n",
    "    min_dim=min(height,width)\n",
    "    max_dim=max(height,width)\n",
    "    \n",
    "    if method=='crop':\n",
    "        return make_image_square(rp.crop_image(image, min_dim, min_dim, origin='center'),'scale')\n",
    "    if method=='scale':\n",
    "        return rp.resize_image(image, (512,512))\n",
    "                    \n",
    "\n",
    "def run_peekaboo(name:str, label:BaseLabel, image:Union[str,np.ndarray],\n",
    "                \n",
    "                #Peekaboo Hyperparameters:\n",
    "                GRAVITY=1e-1/2, # This is the one that needs the most tuning, depending on the prompt...\n",
    "                #   ...usually one of the following GRAVITY will work well: 1e-2, 1e-1/2, 1e-1, or 1.5*1e-1\n",
    "                NUM_ITER=300,       # 300 is usually enough\n",
    "                LEARNING_RATE=1e-5, # Can be larger if not using neural neural textures\n",
    "                BATCH_SIZE=1,       # Doesn't make much difference, larger takes more vram\n",
    "                GUIDANCE_SCALE=100, # The defauly value from the DreamFusion paper\n",
    "                bilateral_kwargs=dict(kernel_size = 3,\n",
    "                                      tolerance = .08,\n",
    "                                      sigma = 5,\n",
    "                                      iterations=40,\n",
    "                                     ),\n",
    "                square_image_method='crop', #Can be either 'crop' or 'scale' - how will we square the input image?\n",
    "                 \n",
    "                )->PeekabooResults:\n",
    "    \n",
    "    image_path='<No image path given>'\n",
    "    if isinstance(image,str):\n",
    "        image_path=image\n",
    "        image=rp.load_image(image)\n",
    "    \n",
    "    assert rp.is_image(image)\n",
    "    assert issubclass(type(label),BaseLabel)\n",
    "    image=rp.as_rgb_image(rp.as_float_image(make_image_square(image,square_image_method)))\n",
    "    rp.tic()\n",
    "    time_started=rp.get_current_date()\n",
    "    \n",
    "    \n",
    "    log_cell('Get Hyperparameters') ########################################################################\n",
    "    icecream.ic(GRAVITY, BATCH_SIZE, NUM_ITER, GUIDANCE_SCALE,  bilateral_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    # log_cell('Alpha Initializer') ########################################################################\n",
    "\n",
    "    p=PeekabooSegmenter(image,labels=[label], name=name, bilateral_kwargs=bilateral_kwargs).to(device)\n",
    "\n",
    "    blur_image=rp.as_numpy_image(p.alphas.bilateral_blur(p.foreground))\n",
    "    print(\"The bilateral blur applied to the input image before/after, to visualize it\")\n",
    "    rp.display_image(rp.tiled_images(rp.labeled_images([rp.as_numpy_image(p.foreground),blur_image],['before','after'])))\n",
    "\n",
    "    p.display();\n",
    "\n",
    "\n",
    "    \n",
    "    # log_cell('Create Optimizers') ########################################################################\n",
    "\n",
    "    params=list(p.parameters())\n",
    "    optim=torch.optim.Adam(params,lr=1e-3)\n",
    "    optim=torch.optim.SGD(params,lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    # log_cell('Create Logs') ########################################################################\n",
    "    global iter_num\n",
    "    iter_num=0\n",
    "    timelapse_frames=[]\n",
    "\n",
    "\n",
    "    # log_cell('Do Training') ########################################################################\n",
    "    preview_interval=NUM_ITER//10 #Show 10 preview images throughout training to prevent output from being truncated\n",
    "    preview_interval=max(1,preview_interval)\n",
    "    log(\"Will show preview images every %i iterations\"%(preview_interval))\n",
    "\n",
    "    try:\n",
    "        display_eta=rp.eta(NUM_ITER)\n",
    "        for _ in range(NUM_ITER):\n",
    "            display_eta(_)\n",
    "            iter_num+=1\n",
    "\n",
    "            alphas=p.alphas()\n",
    "\n",
    "            for __ in range(BATCH_SIZE):\n",
    "                p.randomize_background()\n",
    "                composites=p()\n",
    "                for label, composite in zip(p.labels, composites):\n",
    "                    s.train_step(label.embedding, composite[None], \n",
    "                                 guidance_scale=GUIDANCE_SCALE\n",
    "                                )\n",
    "\n",
    "            ((alphas.sum())*GRAVITY).backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # if not _%100:\n",
    "                    #Don't overflow the notebook\n",
    "                    # clear_output()\n",
    "                if not _%preview_interval: \n",
    "                    timelapse_frames.append(p.display())\n",
    "                    # rp.ptoc()\n",
    "    except KeyboardInterrupt:\n",
    "        log(\"Interrupted early, returning current results...\")\n",
    "        pass\n",
    "\n",
    "                \n",
    "    # rp.ptoc()\n",
    "    results = PeekabooResults(\n",
    "        #The main output is the alphas\n",
    "        alphas=rp.as_numpy_array(alphas),\n",
    "        \n",
    "        #Keep track of hyperparameters used\n",
    "        GRAVITY=GRAVITY,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        NUM_ITER=NUM_ITER,\n",
    "        GUIDANCE_SCALE=GUIDANCE_SCALE,\n",
    "        bilateral_kwargs=bilateral_kwargs,\n",
    "        \n",
    "        #Keep track of the inputs used\n",
    "        label=label,\n",
    "        image=image,\n",
    "        image_path=image_path,\n",
    "        \n",
    "        #Record some extra info\n",
    "        preview_image=p.display(),\n",
    "        timelapse_frames=rp.as_numpy_array(timelapse_frames),\n",
    "        blur_image=blur_image,\n",
    "        height=p.height,\n",
    "        width=p.width,\n",
    "        p_name=p.name,\n",
    "        \n",
    "        git_hash=rp.get_current_git_hash(), \n",
    "        time_started=rp.r._format_datetime(time_started),\n",
    "        time_completed=rp.r._format_datetime(rp.get_current_date()),\n",
    "        device=device,\n",
    "        computer_name=rp.get_computer_name(),\n",
    "    ) \n",
    "    \n",
    "    output_folder = rp.make_folder('peekaboo_results/%s'%name)\n",
    "    output_folder += '/%03i'%len(rp.get_subfolders(output_folder))\n",
    "    \n",
    "\n",
    "    save_peekaboo_results(results,output_folder)\n",
    "    print(\"Please wait - creating a training timelapse\")\n",
    "    clear_output()\n",
    "    rp.display_image_slideshow(timelapse_frames)#This can take a bit of time\n",
    "    print(\"Saved results at %s\"%output_folder)\n",
    "    icecream.ic(name,label,image_path, GRAVITY, BATCH_SIZE, NUM_ITER, GUIDANCE_SCALE,  bilateral_kwargs)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c75b8c-2598-4503-a237-60b8502d9777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.eta: ETR=0:01:30.452514\tETA=0:02:13.673666\tT=0:00:43.221152\tProgress: 97/300"
     ]
    }
   ],
   "source": [
    "results=run_peekaboo('Strawberry Test',\n",
    "                     SimpleLabel('a doggo'),\n",
    "                     \"https://www.shutterstock.com/image-photo/dog-cat-under-plaid-pet-260nw-726710023.jpg\",\n",
    "                     NUM_ITER=300\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34550887-1a02-4767-a41e-2e72e387aaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56245589-9a3f-4cf1-8ee6-fa7b31a48dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
